{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code downloads pollutant, traffic toll, and weather data, and then transforms all the data to a consistent format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import io\n",
    "import xmltodict\n",
    "from MesoPy import Meso\n",
    "from bisect import bisect_left\n",
    "from scipy import interpolate\n",
    "import pickle\n",
    "\n",
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class save_load_pkl: \n",
    "    \"\"\"\n",
    "    Class for saving and loading .pkl files. \n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        self\n",
    "    \n",
    "    def save_obj(obj,name): \n",
    "        \"\"\"\n",
    "        Function for saving .pkl files. \n",
    "        obj = object that you want to save. \n",
    "        name = filepath/filename to which you want to save the object. \n",
    "        \"\"\"\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    def load_obj(name):\n",
    "        \"\"\"\n",
    "        Function for loading data from .pkl files\n",
    "        name = filepath/filename from which you want to load the object. \n",
    "        \"\"\"\n",
    "        with open(name, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deal with pollutant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pollutant_obs(data_destination):\n",
    "    print('I cannot download through URL API requests at the moment (API is down). I will have to do it manually.')\n",
    "    print()\n",
    "    print('Instructions for manual download:')\n",
    "    print(' 1. Navigate to: https://aqs.epa.gov/api')\n",
    "    print(' 2. Set up an account. Enter your email and you will be emailed a password.')\n",
    "    print(' 3. Enter the following information:')\n",
    "    print('    a. Username/Password')\n",
    "    print('    b. Query Type: rawDataNotify')\n",
    "    print('    c. Output Format: DMCSV')\n",
    "    print('    d. Parameter Class: AQI Pollutants')\n",
    "    print('    e. Parameter Code: see above--each value in the pollutant_code list above')\n",
    "    print('    f. Begin Date: 20020101')\n",
    "    print('    g. End Date: 20171231')\n",
    "    print('    h. State Code: 36 - New York')\n",
    "    print('    i. County Code: 081 - Queens')\n",
    "    print('    j. Site: 0124 - QUEENS COLLEGE 2')\n",
    "    print('    k. Duration: H - 5 MINUTE')\n",
    "    print(' 4. Then hit \"SUBMIT\"')\n",
    "    print()\n",
    "    print('You will receive an email when the data is ready for download. Follow instructions to download.')\n",
    "    print('Download all the data files and place them in the following directory:')\n",
    "    print(data_destination)\n",
    "\n",
    "\n",
    "def transform_pollutant_obs(data_path='./data'):\n",
    "    \"\"\"\n",
    "    Function transforms pollutant data to standard format. \n",
    "    INPUT: \n",
    "    data_path = path to directory in which raw pollutant data is stored. Also path to which \n",
    "                pollutant data is saved after transformation. \n",
    "    OUTPUT: \n",
    "    path/file_name to which transformed data is saved (is a .csv file)\n",
    "    \"\"\"\n",
    "    # Initialize DataFrame with time vector... every hour from st_time to ed_time. \n",
    "    st_time = datetime.datetime(year=2002,month=1,day=1,hour=0,minute=0,second=0) \n",
    "    ed_time = datetime.datetime(year=2017,month=12,day=31,hour=23,minute=59,second=59) \n",
    "    delta_time = datetime.timedelta(hours=1)\n",
    "    all_times = np.arange(st_time,ed_time,delta_time)\n",
    "    poll_data = pd.DataFrame({\n",
    "        'time_utc': all_times,\n",
    "    })\n",
    "\n",
    "    # List of files with pollutant data, dictionary to map pollutant long names to short names. \n",
    "    aq_files = glob.glob(os.path.join(data_path,'AQDM*'))\n",
    "    gas_name_dict = {\n",
    "        'Ozone': 'O3',\n",
    "        'Carbon monoxide': 'CO',\n",
    "        'Nitrogen dioxide (NO2)': 'NO2', \n",
    "        'PM2.5 - Local Conditions': 'PM2.5',\n",
    "        'Sulfur dioxide': 'SO2'\n",
    "    }\n",
    "\n",
    "    # Collect important gas data from each gas data file, place in poll_data dataframe. \n",
    "    for file_name in aq_files: \n",
    "\n",
    "        gas_data = pd.read_csv(file_name, dtype=str)[:-1]    \n",
    "        gas_name = gas_data['AQS Parameter Desc'][10]\n",
    "\n",
    "        # Extract useful gas data. \n",
    "        single_gas = pd.DataFrame({})\n",
    "        single_gas['time_utc'] = [datetime.datetime.strptime(date+'_'+hr,'%Y-%m-%d_%H:%M') for date,hr in zip(gas_data['Date GMT'],gas_data['24 Hour GMT'])]\n",
    "        single_gas[gas_name_dict[gas_name]+'_val'] = gas_data['Sample Measurement'].apply(float)\n",
    "        single_gas[gas_name_dict[gas_name]+'_limit'] = gas_data['Detection Limit'].apply(float)\n",
    "        single_gas[gas_name_dict[gas_name]+'_unit'] = gas_data['Units of Measure']\n",
    "        single_gas[gas_name_dict[gas_name]+'_instr'] = gas_data['Method Description'] \n",
    "\n",
    "        # Add useful data to larger structure via a left join on the measurement time. \n",
    "        poll_data = poll_data.join(single_gas.set_index('time_utc'),on='time_utc',how='left')\n",
    "        \n",
    "    # Collect basic information on the air quality measurement station. \n",
    "    station_info = pd.DataFrame({\n",
    "        'lat': gas_data['Latitude'].loc[10],\n",
    "        'lon': gas_data['Longitude'].loc[10],\n",
    "        'state_code': gas_data['State Code'].loc[10],\n",
    "        'county_code': gas_data['County Code'].loc[10],\n",
    "        'site_number': gas_data['Site Num'].loc[10]\n",
    "    },index=[0])\n",
    "\n",
    "    # Save the DataFrame as a CSV. \n",
    "    poll_data.to_csv(os.path.join(data_path,'poll_data_200201010500_201811011833.csv'),na_rep='NaN',index=False)\n",
    "    \n",
    "    # Also save basic station info to CSV. \n",
    "    station_info.to_csv(os.path.join(data_path,'pollutant_station_info.csv'),na_rep='NaN',index=False)\n",
    "    \n",
    "    return os.path.join(data_path,'poll_data_200201010500_201811011833.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot download through URL API requests at the moment (API is down). I will have to do it manually.\n",
      "\n",
      "Instructions for manual download:\n",
      " 1. Navigate to: https://aqs.epa.gov/api\n",
      " 2. Set up an account. Enter your email and you will be emailed a password.\n",
      " 3. Enter the following information:\n",
      "    a. Username/Password\n",
      "    b. Query Type: rawDataNotify\n",
      "    c. Output Format: DMCSV\n",
      "    d. Parameter Class: AQI Pollutants\n",
      "    e. Parameter Code: see above--each value in the pollutant_code list above\n",
      "    f. Begin Date: 20020101\n",
      "    g. End Date: 20171231\n",
      "    h. State Code: 36 - New York\n",
      "    i. County Code: 081 - Queens\n",
      "    j. Site: 0124 - QUEENS COLLEGE 2\n",
      "    k. Duration: H - 5 MINUTE\n",
      " 4. Then hit \"SUBMIT\"\n",
      "\n",
      "You will receive an email when the data is ready for download. Follow instructions to download.\n",
      "Download all the data files and place them in the following directory:\n",
      "./data\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "retrieve_pollutant_obs(data_path)\n",
    "\n",
    "# Transform into consistent format. \n",
    "poll_data_path = transform_pollutant_obs(data_path='./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deal with traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire process in a single parent function: \n",
    "\n",
    "def retrieve_toll_data(toll_data_dir,web_addr='http://web.mta.info/developers/data/bandt/trafficdata.html'): \n",
    "    \"\"\"\n",
    "    Function used to download all traffic data, place it in a simple format. \n",
    "    Input: \n",
    "           toll_data_dir = directory where we want to save all the traffic data.\n",
    "           web_addr = address of webpage where toll data is found. \n",
    "    Output: \n",
    "           save_path_file_name = path/fName for all the downloaded toll data. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Functions: =====================================================================================\n",
    "    # Used in this function. Built specifically for the extraction process.  \n",
    "    \n",
    "    # Download and extract zip files. \n",
    "    def download_extract_zip(retrieval_url,destination):\n",
    "        r = requests.get(retrieval_url)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(destination)\n",
    "        return\n",
    "    \n",
    "    # Download unarchived toll data from a specific .xml file. \n",
    "    def get_xml_traffic_data(xml_file):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "                xml_file = web-address for traffic data xml file. toll_addr links to these addresses. \n",
    "        Output: \n",
    "                list_inst_date,list_inst_id,list_inst_cc,list_inst_ec\n",
    "                Lists of the date, toll_id, cash-count, and etc-count values for each day/facility pair \n",
    "                in the unarchived data. \n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize lists to store instances for this file\n",
    "        li_date_f,li_id_f,li_cc_f,li_ec_f = [],[],[],[]\n",
    "\n",
    "        # Retrieve the data. If this fails, move on to the next file. \n",
    "        contents = requests.get(xml_file).text\n",
    "\n",
    "        # convert into a dictionary. If this isn't possible, it means the file is probably bad so we should\n",
    "        # move on to the next file. \n",
    "        xml_dict = xmltodict.parse(contents)\n",
    "\n",
    "        # Use the dictionary to pull out attribute values for each instance. \n",
    "        for trans_sum_ind in range(len(xml_dict['message']['TransSummary'])): \n",
    "            for facil_ind in range(len(xml_dict['message']['TransSummary'][trans_sum_ind]['facility'])):\n",
    "                list_inst_date.append(xml_dict['message']['TransSummary'][trans_sum_ind]['@Date']);\n",
    "                try: # This work in most cases--unless only one facility on that particular day. \n",
    "                    list_inst_id.append(xml_dict['message']['TransSummary'][trans_sum_ind]['facility'][facil_ind]['@id']);\n",
    "                    list_inst_cc.append(xml_dict['message']['TransSummary'][trans_sum_ind]['facility'][facil_ind]['@cash-count']);\n",
    "                    list_inst_ec.append(xml_dict['message']['TransSummary'][trans_sum_ind]['facility'][facil_ind]['@etc-count']);\n",
    "                except KeyError: \n",
    "                    # If there's only one facility in the 'TransSummary', will not have a list of facilities \n",
    "                    # in xml_dict['message']['TransSummary'][trans_sum_ind]['facility']. Will just have the \n",
    "                    # three fields, '@id', '@cash-count', and '@etc-count'. We need to add the instance for \n",
    "                    # that single facility and then move on to the next day's TransSummary. \n",
    "                    list_inst_id.append(xml_dict['message']['TransSummary'][trans_sum_ind]['facility']['@id']);\n",
    "                    list_inst_cc.append(xml_dict['message']['TransSummary'][trans_sum_ind]['facility']['@cash-count']);\n",
    "                    list_inst_ec.append(xml_dict['message']['TransSummary'][trans_sum_ind]['facility']['@etc-count']);\n",
    "                    break # move to next iteration of the middle loop. \n",
    "        \n",
    "        return li_date_f,li_id_f,li_cc_f,li_ec_f\n",
    "        #return list(map(int, li_date_f)),list(map(int, li_id_f)),list(map(int, li_cc_f)),list(map(int, li_ec_f))\n",
    "\n",
    "    \n",
    "    # Part 1: ========================================================================================\n",
    "    # This section retrieves links to ALL the data I want to collect. \n",
    "    # Archived data is in a .zip file. \n",
    "    # Non-archived data stored in pages with suffixes, .xml\n",
    "\n",
    "    # Create list of accesskey attributes the webpage. These contain the links to the data we want. \n",
    "    html = requests.get(web_addr).text\n",
    "    soup = BeautifulSoup(html,'html5lib')\n",
    "    all_links_unk = soup.find_all('a')\n",
    "    all_links = [str(link)[3:-4] for link in all_links_unk if str(link)[3:7]=='href']\n",
    "    links_splt_gt = [link.split(\">\") for link in all_links]\n",
    "\n",
    "    # Identify entries that contain links to the archived data: these addresses end in \".zip \" \n",
    "    arch_data_links = [lsgt[0][6:-2] for lsgt in links_splt_gt if lsgt[0][-6:-2]=='.zip']\n",
    "\n",
    "    # Identify entries that contain links to un-archived data: these addresses end in \".xml\"\n",
    "    # Note that these links aren't complete--to complete, need to concatenate base_address to the front. \n",
    "    base_address = 'http://web.mta.info/developers/data/bandt/'\n",
    "    unarch_data_links = [base_address+(lsgt[0][6:-1]) for lsgt in links_splt_gt if lsgt[0][-5:-1]=='.xml'] \n",
    "\n",
    "    \n",
    "    # Part 2: ========================================================================================\n",
    "    # Download and extract the archived data from its .zip files. \n",
    "\n",
    "    for aa in arch_data_links: \n",
    "        download_extract_zip(aa,toll_data_dir)\n",
    "    \n",
    "    # Part 3: ========================================================================================\n",
    "    # Place data from xml files in a dataframe. \n",
    "\n",
    "    # List of errors the exception takes care of: \n",
    "    # ExpatError: If there's an error in the .xml file (not my fault, it was written wrong I think?), get \n",
    "                # ExpatError with xml_dict = xmltodict.parse(contents). \n",
    "    # TypeError: If the .xml file has only <message> </message> so xml_dict['message']['TransSummary']\n",
    "                # returns a TypeError. \n",
    "    # Fortunately, no files that trigger these exceptions have data. We can just skip them! \n",
    "    \n",
    "    # Initialize lists to store ALL instances from ALL files. \n",
    "    list_inst_date,list_inst_id,list_inst_cc,list_inst_ec = [],[],[],[]\n",
    "\n",
    "    # Use function to pull data out of .xml files, place data in lists. Then append the lists to \n",
    "    # those that hold data for ALL instances from ALL files. \n",
    "    for xml_file in unarch_data_links:\n",
    "        \n",
    "        try: \n",
    "            li_date_f,li_id_f,li_cc_f,li_ec_f = get_xml_traffic_data(xml_file)\n",
    "        except: \n",
    "            continue\n",
    "            \n",
    "        list_inst_date = list_inst_date+li_date_f\n",
    "        list_inst_id   = list_inst_id+li_id_f\n",
    "        list_inst_cc   = list_inst_cc+li_cc_f\n",
    "        list_inst_ec   = list_inst_ec+li_ec_f\n",
    "\n",
    "    # Place lists in dict, convert into dataframe. \n",
    "    toll_ct_dict = {}\n",
    "    toll_ct_dict['date'],toll_ct_dict['toll_id'],toll_ct_dict['cash-count'],toll_ct_dict['etc-count'] = \\\n",
    "        list_inst_date,list_inst_id,list_inst_cc,list_inst_ec;\n",
    "    unarch_toll_df = pd.DataFrame(data=toll_ct_dict)\n",
    "    \n",
    "    \n",
    "    # Part 4: ========================================================================================\n",
    "    # Now combine the archived and unarchived data into a single dataframe. \n",
    "\n",
    "    # Retrieve the archived data from its storage place. \n",
    "    arch_data_pathFile = os.path.join(toll_data_dir,'TBTA_DAILY_PLAZA_TRAFFIC.csv')\n",
    "    arch_toll_df = pd.read_csv(arch_data_pathFile)\n",
    "\n",
    "    # Re-order attributes in archived dataframe so that it's consistent with the order of the columns in \n",
    "    # the unarchived dataframe. Also re-name each attribute for consistency, to avoid confusing Pandas. \n",
    "    arch_toll_df = arch_toll_df[['DATE','PLAZAID','CASH','ETC']]\n",
    "    arch_toll_df = arch_toll_df.rename(index=str, columns={\"DATE\": \"date\", \"PLAZAID\": \"toll_id\",\"CASH\": \"cash-count\", \"ETC\": \"etc-count\"})\n",
    "\n",
    "    # Join by appending. \n",
    "    unarch_toll_df.append(arch_toll_df,sort=False)\n",
    "\n",
    "    # Save all the toll data in a single CSV file. \n",
    "    save_path_file_name = os.path.join(toll_data_dir,'all_toll_data.csv')\n",
    "    unarch_toll_df.to_csv(path_or_buf=save_path_file_name, sep=',')\n",
    "    \n",
    "    print('All traffic data saved to...',save_path_file_name)\n",
    "    \n",
    "    return save_path_file_name\n",
    "\n",
    "\n",
    "def transform_toll_data(toll_data_path,std_time_list):\n",
    "\n",
    "    def find_eq(a,val):\n",
    "        return [idx for idx in range(len(a)) if a[idx] == val]\n",
    "    \n",
    "    raw_toll = pd.read_csv(toll_data_path,index_col=0,dtype=str)\n",
    "    raw_toll['date'] = raw_toll['date'].apply(lambda x: datetime.datetime.strptime(x,'%m/%d/%Y'))\n",
    "\n",
    "    # Find all unique dates and toll indices. Needed to restructure the toll data. \n",
    "    toll_dates = list(set(raw_toll['date']))\n",
    "    toll_ids   = sorted(list(set(raw_toll['toll_id'])))\n",
    "\n",
    "    # Initialize dictionary to hold restructured data. \n",
    "    struct_toll = {'date':toll_dates}\n",
    "\n",
    "    # Initialize columns that will hold the # of cash and electronic tolls for each toll, for each day. \n",
    "    for tid in toll_ids: \n",
    "        struct_toll['cash_'+str(tid)] = [np.nan]*len(struct_toll['date'])\n",
    "        struct_toll['etc_'+str(tid)] = [np.nan]*len(struct_toll['date'])\n",
    "\n",
    "    # Now fill those initialized columns with the cash and etc-counts for each toll, for each day. \n",
    "    for ind,row in raw_toll.iterrows(): \n",
    "        date,tid,cash,etc = row['date'],row['toll_id'],row['cash-count'],row['etc-count']\n",
    "        struct_ind = find_eq(struct_toll['date'],date)[0]\n",
    "        struct_toll['cash_'+str(tid)][struct_ind] = cash\n",
    "        struct_toll['etc_'+str(tid)][struct_ind] = etc\n",
    "\n",
    "    # Convert to dataframe\n",
    "    struct_toll = pd.DataFrame(struct_toll).sort_values('date')\n",
    "    \n",
    "    # Place the data in the standard format. \n",
    "    toll_data = pd.DataFrame({\n",
    "        'time_utc': list(std_time_list)\n",
    "    })\n",
    "    toll_data['approx_loctime'] = toll_data['time_utc'].apply(lambda x: x-datetime.timedelta(hours=5)) \n",
    "    toll_data['date'] = toll_data['approx_loctime'].apply(lambda x: datetime.datetime(year=x.year,month=x.month,day=x.day))\n",
    "    toll_data = toll_data.merge(struct_toll,how='left',on=['date'])\n",
    "    toll_data = toll_data.drop(labels=['date','approx_loctime'],axis=1)\n",
    "\n",
    "    # Save toll_data to CSV\n",
    "    poll_data.to_csv(os.path.join(data_path,'toll_data_200201010500_201811011833.csv'),na_rep='NaN',index=False)\n",
    "    \n",
    "    return os.path.join(data_path,'toll_data_200201010500_201811011833.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All traffic data saved to... ./data/all_toll_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the data: \n",
    "toll_web_addr = \"http://web.mta.info/developers/data/bandt/trafficdata.html\"\n",
    "raw_toll_path = retrieve_toll_data(data_path,toll_web_addr)\n",
    "\n",
    "# Transform the data: \n",
    "poll_data = pd.read_csv(poll_data_path,dtype=str)\n",
    "toll_data_path = transform_toll_data(raw_toll_path,poll_data['time_utc'].apply(\n",
    "                                     lambda x: pd.Timestamp(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deal with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve observations. \n",
    "def retr_wxobs_synopticlabs(api_key,station_ids,data_path):\n",
    "    \"\"\"\n",
    "    Function to retrieve weather observations from various observation sites. Uses the \n",
    "    MesoWest/SynopticLabs API to retrieve the observations. \n",
    "    \n",
    "    INPUT: \n",
    "        api_key: SynopticLabs api_key. \n",
    "        station_ids: List of four-letter station ids; one for each station for which we want to retrieve\n",
    "                     observations. \n",
    "        data_path: Path to directory in which we want to save the observations. \n",
    "        \n",
    "    OUTPUT: \n",
    "        path_name_list: list of file path/name for all files in which weather data is stored. \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # Generate token\n",
    "    request_generate_token = 'http://api.mesowest.net/v2/auth?apikey='+api_key_synopticlabs\n",
    "    api_out = requests.get(request_generate_token).text\n",
    "    token_dict = json.loads(api_out)\n",
    "    token_synopticlabs = token_dict['TOKEN']\n",
    "\n",
    "    # Set some key parameters for retrieval: \n",
    "    # 1. Start time for observations.  \n",
    "    # 2. End time for observations. \n",
    "    # 3. Variables to retrieve. \n",
    "    # 4. Shortened names for retrieved variables. \n",
    "    # 5. General info about the wx station. \n",
    "    \n",
    "    st_time = '200201010500'\n",
    "    ed_time = '201811011833'\n",
    "\n",
    "    vbl_list = ['date_time','air_temp_set_1','dew_point_temperature_set_1d',\n",
    "                'relative_humidity_set_1','wind_speed_set_1','wind_gust_set_1','wind_direction_set_1',\n",
    "                'sea_level_pressure_set_1d','precip_accum_one_hour_set_1','weather_condition_set_1d',\n",
    "                'visibility_set_1','ceiling_set_1','cloud_layer_1_code_set_1','cloud_layer_2_code_set_1',\n",
    "                'cloud_layer_3_code_set_1']\n",
    "\n",
    "    new_vbl_names = ['time','temp','dewpoint','RH','wind_speed','wind_gust','wind_dir',\n",
    "                    'sl_pres','precip_1hr','wx_cond','visibility',\n",
    "                    'cld_ceiling','cldlayer_1','cldlayer_2','cldlayer_3']\n",
    "\n",
    "    station_attrs = ['STID','ELEVATION','NAME','LONGITUDE','LATITUDE']\n",
    "    \n",
    "    # List to hold file path/name for each station's observations. \n",
    "    path_name_list = []\n",
    "\n",
    "    # Retrieve observations from each station. \n",
    "    for station_id in wx_stations:\n",
    "\n",
    "        # Retrieve the weather observations from API. \n",
    "        m = Meso(token=token_synopticlabs)\n",
    "        data_ts = m.timeseries(stid=station_id, start=st_time, end=ed_time)    \n",
    "\n",
    "        # Place data in a simpler structure, which contains only desired attributes and variables. \n",
    "        obs_dict = {}\n",
    "\n",
    "        obs_dict['station_info'] = {}\n",
    "        for attr in station_attrs: \n",
    "            obs_dict['station_info'][attr] = data_ts['STATION'][0][attr]\n",
    "\n",
    "        temporary_dict = {}\n",
    "        for vbl,new_vbl in zip(vbl_list,new_vbl_names):\n",
    "            temporary_dict[new_vbl] = data_ts['STATION'][0]['OBSERVATIONS'][vbl]\n",
    "        obs_df = pd.DataFrame(temporary_dict)\n",
    "        obs_dict['wx_obs'] = obs_df\n",
    "\n",
    "        obs_dict['units'] = data_ts['UNITS']\n",
    "\n",
    "        obs_dict['qc_summary'] = data_ts['QC_SUMMARY']\n",
    "\n",
    "        # Save the dictionary\n",
    "        file_name = 'wxobs_'+station_id+'_'+st_time+'_'+ed_time+'.npy'\n",
    "        np.save(os.path.join(data_path,file_name),obs_dict) \n",
    "        \n",
    "        # Add file path/name to dictionary. \n",
    "        path_name_list.append(os.path.join(data_path,file_name))\n",
    "\n",
    "    return path_name_list\n",
    "\n",
    "\n",
    "\n",
    "def transform_wx_data(wx_raw_path,station_names,std_time_list):\n",
    "    \"\"\"\n",
    "    Transforms weather data from each station to the standard format. \n",
    "    INPUT: \n",
    "        wx_raw_path: List of file name/paths for raw weather data. \n",
    "        station_names: List of four-letter codes for each station. \n",
    "    OUTPUT: \n",
    "        wx_data_paths: names/paths for csv files that hold transformed data.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def find_closest(myList, myNumber):\n",
    "        \"\"\"\n",
    "        Assumes myList is sorted. Returns closest value to myNumber, index of that value. \n",
    "\n",
    "        If two numbers are equally close, return the smallest number.\n",
    "        \"\"\"\n",
    "        pos = bisect_left(myList, myNumber)\n",
    "        if pos == 0:\n",
    "            return myList[0],pos\n",
    "        if pos == len(myList):\n",
    "            return myList[-1],pos\n",
    "        before = myList[pos - 1]\n",
    "        after = myList[pos]\n",
    "        if after - myNumber < myNumber - before:\n",
    "           return after,pos\n",
    "        else:\n",
    "           return before,pos-1\n",
    "    \n",
    "    try: \n",
    "        all_stations = []\n",
    "        for path in wx_raw_path: \n",
    "            all_stations.append(np.load(path).item())    \n",
    "    except: \n",
    "        kjfk = np.load(os.path.join(data_path,'wxobs_kjfk_200201010500_201811011833.npy')).item()\n",
    "        klga = np.load(os.path.join(data_path,'wxobs_klga_200201010500_201811011833.npy')).item()\n",
    "        knyc = np.load(os.path.join(data_path,'wxobs_knyc_200201010500_201811011833.npy')).item()\n",
    "        all_stations = [kjfk,klga,knyc]\n",
    "\n",
    "    wx_data_paths = []\n",
    "        \n",
    "    for station, station_name in zip(all_stations,station_names):\n",
    "\n",
    "        # Convert timestamp to datetime, sort. \n",
    "        wx_obs = station['wx_obs']\n",
    "        wx_obs['time'] = wx_obs['time'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "        wx_obs = wx_obs.sort_values('time')\n",
    "\n",
    "        # Use nearest-neighbor interpolation to determine whether conditions at each time for which \n",
    "        # we have pollutant data. \n",
    "\n",
    "        # Prepare the data for interpolation...\n",
    "        # 1. Create new time vectors--hours since start of year 2000. Necessary for interpolation\n",
    "        time_del = datetime.timedelta(hours=1)\n",
    "        time_base = datetime.datetime(year=2000,month=1,day=1,hour=0,minute=0,second=0)\n",
    "        h2000_obs = list(wx_obs['time'].apply(lambda x: (x-time_base)/time_del))\n",
    "        h2000_utc = list(std_time_list.apply(lambda x: (x-time_base)/time_del))\n",
    "\n",
    "        # 2. Use label encoder to encode wx_cond... so that we can interpolate.\n",
    "        wx_obs['wx_cond'].fillna(value=pd.np.nan, inplace=True) # Get rid of <None> entries -> replace with NaN. \n",
    "        wx_obs['wx_cond'] = wx_obs['wx_cond'].replace(to_replace=np.nan,value='N/A') # Replace all NaN's with N/A\n",
    "        from sklearn import preprocessing\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        wx_obs['wx_cond_encode'] = le.fit_transform(wx_obs['wx_cond'])\n",
    "\n",
    "        # 3. Prepare the h2000_utc vector for interpolation -> remove all times that aren't within 30 mins of a wx_obs time. \n",
    "        h2000_interp = []\n",
    "        for time in h2000_utc: \n",
    "            time_near,time_near_idx = find_closest(h2000_obs, time)\n",
    "            if time - time_near <= 0.5 and time-time_near > -0.5:\n",
    "                h2000_interp.append(time)\n",
    "\n",
    "        # Convert h2000_interp from hours after 2000 to datetime. \n",
    "        time_utc_interped = [time_base + time_del*h2000 for h2000 in h2000_interp]\n",
    "\n",
    "        # Now interpolate!  \n",
    "        wx_data_interped = pd.DataFrame({'time_utc':time_utc_interped})\n",
    "        obs_to_interp = ['temp', 'dewpoint', 'RH', 'wind_speed', 'wind_gust', 'wind_dir',\n",
    "                         'sl_pres', 'precip_1hr', 'visibility', 'cld_ceiling',\n",
    "                         'cldlayer_1', 'cldlayer_2', 'cldlayer_3', 'wx_cond_encode']\n",
    "\n",
    "        for obs_name in obs_to_interp: \n",
    "            f = interpolate.interp1d(h2000_obs,list(wx_obs[obs_name]),kind='nearest',bounds_error=False,fill_value=np.nan)\n",
    "            wx_data_interped[obs_name] = f(np.asarray(h2000_interp))\n",
    "\n",
    "        # Convert encoded wx_cond to wx_cond names. \n",
    "        wx_data_interped['wx_cond'] = le.inverse_transform(wx_data_interped['wx_cond_encode'].apply(int))\n",
    "\n",
    "        # Use left join to make format consistent with pollutant data. \n",
    "        wx_data = pd.DataFrame({'time_utc': list(std_time_list)})\n",
    "        wx_data = wx_data.merge(wx_data_interped,how='left',on='time_utc',sort=True)\n",
    "\n",
    "        # Save to csv\n",
    "        wx_data.to_csv(os.path.join(data_path,'wx_data_'+station_name+'_200201010500_201811011833.csv'),na_rep='NaN',index=False)\n",
    "\n",
    "        wx_data_paths.append(os.path.join(data_path,'wx_data_'+station_name+'_200201010500_201811011833.csv'))\n",
    "        \n",
    "    return wx_data_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data. \n",
    "from api_keys import api_key_synopticlabs\n",
    "wx_stations = ['kjfk','klga','knyc']\n",
    "wx_raw_path = retr_wxobs_synopticlabs(api_key_synopticlabs,wx_stations,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "wx_data_paths = transform_wx_data(wx_raw_path,wx_stations,poll_data['time_utc'].apply(\n",
    "                                     lambda x: pd.Timestamp(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
